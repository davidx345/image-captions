{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis\n",
    "\n",
    "In this notebook, we will analyze the performance of our trained image captioning model. We will visualize the generated captions and compare them with the actual captions from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from src.models.caption_model import CaptionModel\n",
    "from src.data.dataset import Dataset\n",
    "from src.utils.vocabulary import Vocabulary\n",
    "\n",
    "# Load the trained model\n",
    "model = CaptionModel()  # Initialize the model\n",
    "model.load_weights('models/saved_models/caption_model_weights.h5')  # Load the weights\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Dataset()  # Initialize the dataset\n",
    "vocab = Vocabulary()  # Initialize the vocabulary\n",
    "dataset.load_data()  # Load the data\n",
    "\n",
    "# Function to display images and captions\n",
    "def display_image_and_captions(image, actual_caption, generated_caption):\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Actual: {actual_caption}\\nGenerated: {generated_caption}')\n",
    "    plt.show()\n",
    "\n",
    "# Analyze a few samples\n",
    "num_samples = 5\n",
    "for i in range(num_samples):\n",
    "    # Get a sample image and its actual caption\n",
    "    image, actual_caption = dataset.get_sample(i)\n",
    "    \n",
    "    # Generate a caption for the image\n",
    "    generated_caption = model.generate_caption(image)\n",
    "    \n",
    "    # Display the image and captions\n",
    "    display_image_and_captions(image, actual_caption, generated_caption)\n",
    "\n",
    "# Evaluate the model using BLEU and METEOR scores\n",
    "from src.training.evaluate import evaluate_model\n",
    "bleu_score, meteor_score = evaluate_model(model, dataset)\n",
    "print(f'BLEU Score: {bleu_score}')\n",
    "print(f'METEOR Score: {meteor_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}